{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd4SVfDxz5aw"
   },
   "source": [
    "# Localisation et détection d'objet\n",
    "\n",
    "Dans ce TP, nous allons mettre en pratique certaines des méthodes présentées en cours pour localiser des objets dans une image.\n",
    "\n",
    "En localisation et détection, on cherche à déterminer la position d'un objet, ainsi que sa classe, sous la forme d'une boîte englobante de largeur $b_w$ et hauteur $b_h$, et dont le centre a pour coordonnées le point $(b_x, b_y)$. \n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1_jHHv6ZDe-3Xz25jIZ6o177laBEfmMRR\" style=\"width:500;height:300px;\"></center>\n",
    "<caption><center> Figure 1: Modèle de boîte englobante utilisé pour la localisation </center></caption>\n",
    "\n",
    "Le problème de localisation considère qu'un seul objet est présent sur l'image, alors que le problème de détection cherche à déterminer l'ensemble des objets présents sur l'image.\n",
    "\n",
    "\n",
    "\n",
    "Pour commencer, récupérez les images de la base de données :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12604,
     "status": "ok",
     "timestamp": 1612804199570,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -60
    },
    "id": "2ZjveWpbuNeV",
    "outputId": "643a7843-9940-458a-b65f-f6c464d8d2d7"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/axelcarlier/wildlife.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF6aRZLE2-Yl"
   },
   "source": [
    "La base de données comporte 4 classes, pour les 4 animaux suivants : buffle, éléphant, rhinocéros et zèbre.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1sej2InBiQDEmpk2RA7S2dLRsJvF_UszE\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=1K8cO4plzVIXO1MC4mkRcFIFjgspHtOIm\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=15pkHpPW_VR1joOyPryS1pavOTqmoNR88\" width=200>\n",
    "<img src=\"https://drive.google.com/uc?id=19zHZn-A_j8Sx0G3V2SwR_YNKF0dGI1NA\" width=200></center>\n",
    "<caption><center> Figure 2: Exemples d'images de la base de données </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxZ6cVouz9Lp"
   },
   "source": [
    "# Localisation et classification d'objet\n",
    "\n",
    "Dans cette partie, nous allons nous concentrer sur le problème de la localisation d'un seul objet par classe. Pour le cas où il y aurait plusieurs objets sur la même image, nous considérerons uniquement l'objet dont la boîte englobante occupe la plus grande surface sur l'image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKRm5oT-_Qsw"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "La fonction ci-dessous permet de charger les données et les formater pour la classification. Prenez le temps de regarder un peu le format des labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTl_zd240IbV"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# La base de données contient 376 images de chacune des 4 classes\n",
    "DATASET_SIZE = 376*4\n",
    "# Nous choisissons ici la dimension dans laquelle nous allons redimensionner les images\n",
    "# 64x64 est une dimension assez faible mais qui nous permettra d'avoir des expériences plus rapides\n",
    "# 128x128 ou 256x256 permettraient d'avoir de meilleurs résultats mais au prix de plusieurs heures d'entraînement\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "def load_data_localization(image_size):\n",
    "  # Chemin vers la base de données\n",
    "  ds_path = \"./wildlife/\"\n",
    "  # Chemins vers les données des 4 différentes classes\n",
    "  paths = [ds_path + \"buffalo/\", ds_path + \"elephant/\", ds_path + \"rhino/\", ds_path + \"zebra/\"]\n",
    "  # Indice d'ajout de données dans les variables x et y \n",
    "  i = 0\n",
    "  # Préparation des structures de données pour x et y\n",
    "  x = np.zeros((DATASET_SIZE, image_size, image_size, 3))\n",
    "  y = np.empty((DATASET_SIZE, 9)) # 9 = 1 + 4 + 4 : présence / boîte englobante / classes\n",
    "    \n",
    "  # Parcours des chemins de chacune des classes\n",
    "  for path in paths:\n",
    "\n",
    "    # Parcours des fichiers (classés) du répertoire\n",
    "    dirs = os.listdir(path)\n",
    "    dirs.sort()\n",
    "\n",
    "    for item in dirs:\n",
    "      #print(path+item)\n",
    "      if os.path.isfile(path + item):\n",
    "        # Extraction de l'extension du fichier \n",
    "        extension =item.split(\".\")[1]\n",
    "\n",
    "        if extension==\"jpg\" or extension==\"JPG\":\n",
    "          # Image : on va remplir la variable x\n",
    "          # Lecture de l'image\n",
    "          img = Image.open(path + item)\n",
    "          # Mise à l'échelle de l'image\n",
    "          img = img.resize((image_size,image_size), Image.ANTIALIAS)\n",
    "          # Remplissage de la variable x\n",
    "          x[i] = np.asarray(img)\n",
    "\n",
    "        elif extension==\"txt\":\n",
    "          # Fichier Texte : coordonnées de boîtes englobantes pour remplir y\n",
    "          labels = open(path + item, \"r\")\n",
    "          # Récupération des lignes du fichier texte\n",
    "          labels= labels.read().split('\\n')\n",
    "          # Si la dernière ligne est vide, la supprimer \n",
    "          if labels[-1]==\"\":\n",
    "            del labels[-1]\n",
    "\n",
    "          # Indice de la boîte englobante de surface maximale\n",
    "          j_max = 0\n",
    "          if len(labels) > 1:\n",
    "            aire_max = 0 # Surface de la boîte englobante de surface maximale\n",
    "            # Parcours des boîtes englobantes des objets présents sur l'image\n",
    "            for j in range(len(labels)):\n",
    "              # Calcul de l'aire de la boîte englobante courante\n",
    "              aire = float(labels[j].split()[3]) * float(labels[j].split()[4])\n",
    "              # Mise à jour de la boîte englobante de surface maximale, si nécessaire\n",
    "              if aire > aire_max:\n",
    "                aire_max = aire\n",
    "                j_max = j    \n",
    "\n",
    "          # Un objet est présent sur l'image (presence = 1)\n",
    "          presence = np.array([1], dtype=\"i\")\n",
    "          # \"One-hot vector\" représentant les probabilités de classe\n",
    "          classes = np_utils.to_categorical(labels[j_max].split()[0], num_classes=4)\n",
    "          # Coordonnées de la boîte englobante de surface maximale\n",
    "          coordonnees = np.array(labels[j_max].split()[1:], dtype=\"f\")\n",
    "          # Remplissage de la variable y\n",
    "          y[i, 0] = presence\n",
    "          y[i, 1:5] = coordonnees\n",
    "          y[i, 5:] = classes\n",
    "          \n",
    "          #plt.imshow()\n",
    "          i = i + 1\n",
    "        else:\n",
    "          print(\"extension trouvée : \", extension)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "x,y = load_data_localization(IMAGE_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnlljZWc_i1L"
   },
   "source": [
    "Séparation des données en ensembles d'entraînement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJLRiuFX_VPL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Pour améliorer l'entraînement, on peut centrer-réduire les coordonnées des bounding boxes\n",
    "y_std = np.std(y_train, axis=0)\n",
    "y_mean = np.mean(y_train, axis=0)\n",
    "y_train[...,1:5] = (y_train[...,1:5] - y_mean[1:5])/y_std[1:5]\n",
    "y_val[...,1:5] = (y_val[...,1:5] - y_mean[1:5])/y_std[1:5]\n",
    "\n",
    "# Et normaliser les valeurs de couleur\n",
    "x_train = x_train/255\n",
    "x_val = x_val/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4wQYq3AKnA"
   },
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMntCEgkANMg"
   },
   "source": [
    "Calcul du coefficient de Jaccard (intersection sur union) entre boîtes englobantes réelles et prédites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVk9cB1WAMUK"
   },
   "outputs": [],
   "source": [
    "def compute_iou(y_true, y_pred):\n",
    "  ### \"Dénormalisation\" des coordonnées des boîtes englobantes\n",
    "  pred_box_xy = y_pred[..., 0:2]* y_std[0:2] + y_mean[0:2]\n",
    "  true_box_xy = y_true[..., 0:2]* y_std[0:2] + y_mean[0:2]\n",
    "\n",
    "  ### \"Dénormalisation\" des largeur et hauteur des boîtes englobantes\n",
    "  pred_box_wh = y_pred[..., 2:4] * y_std[2:4] + y_mean[2:4]\n",
    "  true_box_wh = y_true[..., 2:4] * y_std[2:4] + y_mean[2:4]\n",
    "    \n",
    "  # Calcul des coordonnées minimales et maximales des boiptes englobantes réelles\n",
    "  true_wh_half = true_box_wh / 2.\n",
    "  true_mins    = true_box_xy - true_wh_half\n",
    "  true_maxes   = true_box_xy + true_wh_half\n",
    "  \n",
    "  # Calcul des coordonnées minimales et maximales des boiptes englobantes prédites\n",
    "  pred_wh_half = pred_box_wh / 2.\n",
    "  pred_mins    = pred_box_xy - pred_wh_half\n",
    "  pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "  \n",
    "  # Détermination de l'intersection des boîtes englobantes\n",
    "  intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "  intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "  intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "  intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "  \n",
    "  # Aire des boîtes englobantes prédites et réelles\n",
    "  true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "  pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "  # Aire de l'union des boîtes prédites et réelles\n",
    "  union_areas = pred_areas + true_areas - intersect_areas\n",
    "\n",
    "  iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "  return iou_scores\n",
    "\n",
    "def iou():\n",
    "  def iou_metrics(y_true, y_pred):\n",
    "    return compute_iou(y_true, y_pred)\n",
    "  iou_metrics.__name__= \"IoU\"\n",
    "  return iou_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vidE3XHlAkst"
   },
   "source": [
    "Visualisation des données et labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yotZHKgAiV1"
   },
   "outputs": [],
   "source": [
    "# Si seuls x et y sont indiqués, on tire au hasard un numéro d'image et on affiche le label y associé  à l'image\n",
    "# Si un 2e y, nommé y_pred, est indiqué, alors les deux labels sont affichés côte à côte, afin de pouvoir les comparer\n",
    "# Enfin on peut également indiquer l'id de l'image que l'on souhaite visualiser.\n",
    "def print_data_localisation(x, y, y_pred=[], id=None, image_size=IMAGE_SIZE):\n",
    "  if id==None:\n",
    "    # Tirage aléatoire d'une image dans la base\n",
    "    num_img = np.random.randint(x.shape[0]-1)\n",
    "  else:\n",
    "    num_img = id\n",
    "\n",
    "  img = x[num_img]\n",
    "  lab = y[num_img]\n",
    "\n",
    "  colors = [\"blue\", \"yellow\", \"red\", \"orange\"] # Différentes couleurs pour les différentes classes\n",
    "  classes = [\"Buffalo\", \"Elephant\", \"Rhino\", \"Zebra\"]\n",
    "\n",
    "  if np.any(y_pred):\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "  # Affichage de l'image\n",
    "  plt.imshow(img)\n",
    "  # Détermination de la classe\n",
    "  class_id = np.argmax(lab[5:])\n",
    "\n",
    "  # Détermination des coordonnées de la boîte englobante dans le repère image\n",
    "  ax = (lab[1]*y_std[1] + y_mean[1]) * image_size\n",
    "  ay = (lab[2]*y_std[2] + y_mean[2]) * image_size\n",
    "  width = (lab[3]*y_std[3] + y_mean[3]) * image_size\n",
    "  height = (lab[4]*y_std[4] + y_mean[4]) * image_size\n",
    "  #print(\"x: {}, y: {}, w: {}, h:{}\".format(ax,ay,width, height))\n",
    "  # Détermination des extrema de la boîte englobante\n",
    "  p_x = [ax-width/2, ax+width/2]\n",
    "  p_y = [ay-height/2, ay+height/2]\n",
    "  # Affichage de la boîte englobante, dans la bonne couleur\n",
    "  plt.plot([p_x[0], p_x[0]],p_y,color=colors[class_id])\n",
    "  plt.plot([p_x[1], p_x[1]],p_y,color=colors[class_id])\n",
    "  plt.plot(p_x,[p_y[0],p_y[0]],color=colors[class_id])\n",
    "  plt.plot(p_x,[p_y[1],p_y[1]],color=colors[class_id])\n",
    "  plt.title(\"Vérité Terrain : Image {} - {}\".format(num_img, classes[class_id]))\n",
    "\n",
    "  if np.any(y_pred):\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Affichage de l'image\n",
    "    plt.imshow(img)\n",
    "    lab = y_pred[num_img]\n",
    "    # Détermination de la classe\n",
    "    class_id = np.argmax(lab[5:])\n",
    "\n",
    "    # Détermination des coordonnées de la boîte englobante dans le repère image\n",
    "    ax = (lab[1]*y_std[1] + y_mean[1]) * image_size\n",
    "    ay = (lab[2]*y_std[2] + y_mean[2]) * image_size\n",
    "    width = (lab[3]*y_std[3] + y_mean[3]) * image_size\n",
    "    height = (lab[4]*y_std[4] + y_mean[4]) * image_size\n",
    "    #print(\"x: {}, y: {}, w: {}, h:{}\".format(ax,ay,width, height))\n",
    "    # Détermination des extrema de la boîte englobante\n",
    "    p_x = [ax-width/2, ax+width/2]\n",
    "    p_y = [ay-height/2, ay+height/2]\n",
    "    # Affichage de la boîte englobante, dans la bonne couleur\n",
    "    plt.plot([p_x[0], p_x[0]],p_y,color=colors[class_id])\n",
    "    plt.plot([p_x[1], p_x[1]],p_y,color=colors[class_id])\n",
    "    plt.plot(p_x,[p_y[0],p_y[0]],color=colors[class_id])\n",
    "    plt.plot(p_x,[p_y[1],p_y[1]],color=colors[class_id])\n",
    "    plt.title(\"Prédiction : Image {} - {}\".format(num_img, classes[class_id]))\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "for i in range(100):#x.shape[0]):\n",
    "    print_data_localisation(x_train, y_train, image_size=IMAGE_SIZE, id=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6O7R3qnCtlN"
   },
   "outputs": [],
   "source": [
    "def plot_training_analysis(history, metric='loss'):    \n",
    "\n",
    "  loss = history.history[metric]\n",
    "  val_loss = history.history['val_' + metric]\n",
    "\n",
    "  epochs = range(len(loss))\n",
    "\n",
    "  plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training ' + metric)\n",
    "  plt.plot(epochs, val_loss, 'g', label='Validation ' + metric)\n",
    "  plt.title('Training and validation ' + metric)\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F_l_yNlDapa"
   },
   "source": [
    "## Travail à faire\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6D31NGPNyJp"
   },
   "source": [
    "<center> <img src=\"https://drive.google.com/uc?id=1YzCZe4pgnjJDVGklAaCHZ7HhlPJsadg9\" width=500></center>\n",
    "<caption><center> Figure 3: Illustration de l'architecture du réseau à construire.  </center></caption>\n",
    "\n",
    "Complétez les codes qui vous sont fournis pour obtenir un algorithme de localisation. \n",
    "Vous pouvez utiliser n'importe quelle base convolutive de votre choix, en revanche vous devrez porter une attention particulière à la couche de sortie.\n",
    "\n",
    "Vous allez en fait produire 3 sorties différentes : une caractérisant la présence d'un objet, une autre fournissant les coordonnées de la boîte englobante, et enfin une dernière effectuant la classification.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1bnh8zU7Os-w-5TT8hV4xDoThKQc-Ywc2\" width=500></center>\n",
    "<caption><center> Figure 4: Illustration des fonctions de coût à utiliser pour l'entraînement. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPOXiJ7hDcbr"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def create_model_localisation(input_shape=(64, 64, 3)):\n",
    "\n",
    "  input_layer = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "  conv1 = Conv2D(...)(input_layer)\n",
    "  # Convolutions, Pooling, Dense,  à vous de former votre réseau\n",
    "  # Votre dernière couche doit mettre à jour une variable x, réutilisée dans les couches de sortie ci-dessous\n",
    "  x = ...\n",
    "\n",
    "  output_p = Dense(..., activation=..., name='p')(x)   # Sortie caractérisant la présence d'un objet\n",
    "  output_coord = Dense(..., activation=..., name='coord')(x) # Sortie caractérisant les coordonnées de boîte englobante\n",
    "  output_class = Dense(..., activation=..., name='classes')(x) # Sortie caractérisant les probabilités de classe\n",
    "  \n",
    "  output= [output_p, output_coord, output_class]\n",
    "  model = Model(input_layer, output)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYoVYfQpotjy"
   },
   "source": [
    "Pour entraîner votre réseau, vous allez donc devoir associer une fonction de coût à chacune des sorties du réseau. La fonction de coût totale sera la somme des trois fonctions de coût précédemment définies, pondérées par des poids définis dans la variable *loss_weights*.\n",
    "\n",
    "**Prenez le temps de tester différentes valeurs de *loss_weights* en fonction de l'évolution des métriques que vous observerez pendant l'entraînement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_ewlCn5Rovm"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=16\n",
    "model = create_model_localisation()\n",
    "opt = Adam(learning_rate=3e-4)  \n",
    "\n",
    "# Ici mettre, dans l'ordre, les fonctions de coût associées à chacune des sorties\n",
    "loss=[..., ..., ...]\n",
    "# On va associer une métrique à chaque sortie : l'accuracy pour les deux classifications, \n",
    "# et l'IoU définie plus tôt pour la qualité des boîtes englobantes. \n",
    "metrics=[ ['accuracy'], [iou()], ['accuracy']]\n",
    "loss_weights = [1, 5, 1]\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics,\n",
    "              loss_weights=loss_weights\n",
    "              )\n",
    "\n",
    "history = model.fit(x_train, [y_train[:,0], y_train[:,1:5], y_train[:,5:9]],\n",
    "              epochs=30,\n",
    "              batch_size=batch_size,            \n",
    "              validation_data=(x_val, [y_val[:,0], y_val[:,1:5], y_val[:,5:9]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc-sGe8LNzA-"
   },
   "outputs": [],
   "source": [
    "# Analyse des résultats : courbes d'évolution de la fonction de perte, et de l'IoU des boîtes englobantes ainsi que de l'accuracy des classes prédites\n",
    "plot_training_analysis(history, metric='loss')\n",
    "plot_training_analysis(history, metric='coord_IoU')\n",
    "plot_training_analysis(history, metric='classes_accuracy')\n",
    "\n",
    "# Prédiction des données de validation\n",
    "y_pred_presence, y_pred_coords, y_pred_classes = model.predict(x_val)\n",
    "y_pred = np.zeros(y_val.shape)\n",
    "for i in range(y_pred.shape[0]):\n",
    "  y_pred[i, 0] = y_pred_presence[i]\n",
    "  y_pred[i, 1:5] = y_pred_coords[i]\n",
    "  y_pred[i, 5:9] = y_pred_classes[i]\n",
    "\n",
    "# Affichage des résultats sur plusieurs images\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=2, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=3, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=7, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=25, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=16, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=18, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=24, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=15, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiFO6fBns-OI"
   },
   "source": [
    "En pratique, il est délicat de trouver une bonne combinaison des fonctions de perte tel que vous l'avez fait sur les cellules précédentes. L'entropie croisée et l'erreur quadratique moyenne donnent des valeurs trop différentes pour être combinables efficacement.\n",
    "\n",
    "Une variante, peut-être plus simple à faire fonctionner, est d'utiliser uniquement l'erreur quadratique moyenne comme perte pour toutes les sorties. C'est cette variante qui est implémentée dans l'algorithme YOLO, dont nous implémenterons une variante dans le prochain TP.\n",
    "Testez cette solution ci-dessous. Comme sur l'exercice précédent, n'hésitez pas à faire varier le poids des différents éléments de la fonction de coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttGCeqz0Dc3y"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=16\n",
    "model = create_model_localisation()\n",
    "opt = Adam(learning_rate=3e-4)  \n",
    "\n",
    "loss = [..., ..., ...]\n",
    "metrics =[ ['accuracy'], [iou()], ['accuracy']]\n",
    "loss_weights = [...]\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics,\n",
    "              loss_weights=loss_weights\n",
    "              )\n",
    "history = model.fit(x_train, [y_train[:,0], y_train[:,1:5], y_train[:,5:9]],\n",
    "              epochs=30,\n",
    "              batch_size=batch_size,            \n",
    "              validation_data=(x_val, [y_val[:,0], y_val[:,1:5], y_val[:,5:9]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKlN9-clJMrI"
   },
   "outputs": [],
   "source": [
    "# Analyse des résultats : courbes d'évolution de la fonction de perte, et de l'IoU des boîtes englobantes ainsi que de l'accuracy des classes prédites\n",
    "plot_training_analysis(history, metric='loss')\n",
    "plot_training_analysis(history, metric='coord_IoU')\n",
    "plot_training_analysis(history, metric='classes_accuracy')\n",
    "\n",
    "# Prédiction des données de validation\n",
    "y_pred_presence, y_pred_coords, y_pred_classes = model.predict(x_val)\n",
    "y_pred = np.zeros(y_val.shape)\n",
    "for i in range(y_pred.shape[0]):\n",
    "  y_pred[i, 0] = y_pred_presence[i]\n",
    "  y_pred[i, 1:5] = y_pred_coords[i]\n",
    "  y_pred[i, 5:9] = y_pred_classes[i]\n",
    "\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=2, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=3, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=7, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=25, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=16, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=18, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=24, image_size=IMAGE_SIZE)\n",
    "print_data_localisation(x_val, y_val, y_pred = y_pred, id=15, image_size=IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stQpmnmAt_bf"
   },
   "source": [
    "Compte-tenu de la taille réduite de la base de données, les résultats ne sont pas mal du tout ! On observe quelques confusions entre certaines classes mais les prédictions sont souvent intéressantes.\n",
    "\n",
    "Il devrait cependant subsister un fort surapprentissage à ce stade. Comme nous l'avons vu dans de précédents TPs, vous avez plusieurs possibilités qui s'offrent à vous pour le corriger : \n",
    "\n",
    "\n",
    "*   Régularisation par *weight decay* (utilisation de *kernel_regularizer* sur les couches de votre réseau)\n",
    "*   Augmentation de la base de données. Vous pouvez pour cela vous appuyer sur l'exemple fourni en TP précédent, avec une classe *Sequence* et l'utilisation de la librairie *Albumentation*.\n",
    "*   Utilisation de *transfer learning* : partant d'un réseau entraîné sur ImageNet (qui contient de nombreuses classes d'animaux), vous bénéficieriez de filtres très généraux qui aiderait à limiter le surapprentissage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code fourni pour vous aider à mettre en place l'augmentation de données\n",
    "\n",
    "On met à jour la version d'Albumentations.ai pour bénéficier des dernières fonctionnalités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U albumentations\n",
    "!echo \"$(pip freeze | grep albumentations) is successfully installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from albumentations import (Compose, RandomBrightness, RandomContrast, RandomGamma, ShiftScaleRotate, RandomSizedBBoxSafeCrop)\n",
    "import albumentations as A\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    ShiftScaleRotate(p=0.5),\n",
    "    RandomContrast(limit=0.2, p=0.5),\n",
    "    RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    RandomBrightness(limit=0.2, p=0.5)\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "class WildLifeSequence(Sequence):\n",
    "    # Initialisation de la séquence avec différents paramètres\n",
    "    def __init__(self, x_set, y_set, batch_size,augmentations):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.classes = [\"Buffalo\", \"Elephant\", \"Rhino\", \"Zebra\"]\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augmentations\n",
    "        self.indices1 = np.arange(x_set.shape[0]) \n",
    "        np.random.shuffle(self.indices1) # Les indices permettent d'accéder\n",
    "        # aux données et sont randomisés à chaque epoch pour varier la composition\n",
    "        # des batches au cours de l'entraînement\n",
    "\n",
    "    # Fonction calculant le nombre de pas de descente du gradient par epoch\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    # Il y a des problèmes d'arrondi dans les conversions de boîtes englobantes \n",
    "    # internes à la librairie Albumentations\n",
    "    # Pour les contourner, si les boîtes sont trop proches des bords, on les érode un peu\n",
    "    def erode_bounding_box(self, box):\n",
    "        epsilon = 0.01\n",
    "        \n",
    "        xmin = max(box[0] - box[2]/2, epsilon)\n",
    "        ymin = max(box[1] - box[3]/2, epsilon)\n",
    "        xmax = min(box[0] + box[2]/2, 1-epsilon)\n",
    "        ymax = min(box[1] + box[3]/2, 1-epsilon)\n",
    "        \n",
    "        cx = xmin + (xmax - xmin)/2\n",
    "        cy = ymin + (ymax - ymin)/2\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        return np.array([cx, cy, width, height])\n",
    "    \n",
    "    # Application de l'augmentation de données à chaque image du batch et aux\n",
    "    # boîtes englobantes associées\n",
    "    def apply_augmentation(self, bx, by):\n",
    "\n",
    "        batch_x = np.zeros((bx.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "        batch_y = by\n",
    "        \n",
    "        # Pour chaque image du batch\n",
    "        for i in range(len(bx)):\n",
    "            bboxes = []\n",
    "            box = by[i,1:5]\n",
    "            # Dénormalisation des coordonnées de boites englobantes\n",
    "            box[0] = (box[0]*y_std[1] + y_mean[1])\n",
    "            box[1] = (box[1]*y_std[2] + y_mean[2])\n",
    "            box[2] = (box[2]*y_std[3] + y_mean[3])\n",
    "            box[3] = (box[3]*y_std[4] + y_mean[4])\n",
    "            box = self.erode_bounding_box(box)\n",
    "            bboxes.append(box)\n",
    "            \n",
    "            class_labels = []\n",
    "            class_id = np.argmax(by[i, 5:])\n",
    "            class_labels.append(self.classes[class_id])\n",
    "\n",
    "            img = bx[i]\n",
    "\n",
    "            # Application de l'augmentation à l'image et aux masques\n",
    "            transformed = self.augment(image=img.astype('float32'), bboxes=bboxes, class_labels=class_labels)\n",
    "            batch_x[i] = transformed['image']\n",
    "            batch_y_transformed = transformed['bboxes']\n",
    "                \n",
    "            # Renormalisation des coordonnées de boîte englobante transformée\n",
    "            batch_y[i, 1] = (batch_y_transformed[0][0] - y_mean[1])/y_std[1]\n",
    "            batch_y[i, 2] = (batch_y_transformed[0][1] - y_mean[2])/y_std[2]\n",
    "            batch_y[i, 3] = (batch_y_transformed[0][2] - y_mean[3])/y_std[3]\n",
    "            batch_y[i, 4] = (batch_y_transformed[0][3] - y_mean[4])/y_std[4]\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    # Fonction appelée à chaque nouveau batch : sélection et augmentation des données\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[self.indices1[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        batch_y = self.y[self.indices1[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        batch_x, batch_y = self.apply_augmentation(batch_x, batch_y)\n",
    "        \n",
    "        batch_y = np.array(batch_y)\n",
    "        return np.array(batch_x), [batch_y[:,0], batch_y[:,1:5], batch_y[:,5:9]]\n",
    "\n",
    "    # Fonction appelée à la fin d'un epoch ; on randomise les indices d'accès aux données\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'une Sequence\n",
    "train_gen = WildLifeSequence(x_train, y_train, 16, augmentations=AUGMENTATIONS_TRAIN)\n",
    "\n",
    "# Pour tester la séquence, nous sélectionnons les éléments du premier batch et les affichons\n",
    "batch_x, batch_y = train_gen.__getitem__(0)\n",
    "\n",
    "y_batch = np.zeros((batch_y[0].shape[0],9))\n",
    "\n",
    "for i in range(batch_y[0].shape[0]):\n",
    "  y_batch[i, 0] = batch_y[0][i]\n",
    "  y_batch[i, 1:5] = batch_y[1][i]\n",
    "  y_batch[i, 5:9] = batch_y[2][i]\n",
    "\n",
    "print_data_localisation(batch_x, y_batch, image_size=IMAGE_SIZE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP9Ho8XgYf4uc8r8fEoDKzU",
   "collapsed_sections": [],
   "name": "IAM2020 - TP5 - Détection d'objet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
